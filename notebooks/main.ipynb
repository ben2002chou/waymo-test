{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUKCixmmV88i"
      },
      "source": [
        "# main.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ysgJGzV88k"
      },
      "source": [
        "## 1. Enviroment verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mQRMDZZwV88l",
        "outputId": "4bf5bd63-3d52-4116-d24a-48310d33af3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab\n",
            "No GPU detected\n",
            "Current path: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1.1 Use Colab or other machine\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running on CoLab')\n",
        "    use_colab = True\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    use_colab = False\n",
        "\n",
        "# 1.2 Use GPU or not\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if len(gpus) > 0:\n",
        "    print('GPU detected')\n",
        "    use_gpu = True\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"GPU {i} - Name: {gpu.name}, Type: {gpu.device_type}\")\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print('No GPU detected')\n",
        "    use_gpu = False\n",
        "\n",
        "del gpus\n",
        "\n",
        "# 1.3 Get current path\n",
        "current_path = os.getcwd()\n",
        "print(f'Current path: {current_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMAMwKvqV88m"
      },
      "source": [
        "## 2. Setup your src package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n1sZbCbmV88n",
        "outputId": "ffc1f680-31c9-45cc-e00c-f804b56b2668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 585 ms, sys: 20.6 ms, total: 606 ms\n",
            "Wall time: 608 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if use_colab and not (os.path.exists('/content/requirements.txt') and os.path.exists('/content/src')):\n",
        "    !ACCESS_TOKEN=\"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\"&&\\\n",
        "    BRANCHE_NAME=\"feature/dvc-pipeline\"&&\\\n",
        "    git clone -b ${BRANCHE_NAME} https://${ACCESS_TOKEN}@github.com/tc-huang/waymo-project.git\n",
        "    !mv /content/waymo-project/src /content/src\n",
        "    !mv /content/waymo-project/env/requirements_docker.txt /content/requirements.txt\n",
        "    !rm -r /content/waymo-project\n",
        "    !pip install -r /content/requirements.txt\n",
        "    if use_gpu:\n",
        "        pass\n",
        "        # !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "        # !python rapidsai-csp-utils/colab/pip-install.py\n",
        "    print(\"Need to restart runtime!!!\")\n",
        "\n",
        "from waymo_open_dataset import v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhwIToGwV88n"
      },
      "source": [
        "#### Network speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sfNfb7h4V88n",
        "outputId": "f288c053-c77c-4b21-b52a-e62c0dfb8aea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: speedtest-cli in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Download Speed: 3218.31 Mbps\n",
            "Upload Speed: 547.50 Mbps\n"
          ]
        }
      ],
      "source": [
        "!pip install speedtest-cli\n",
        "from src.utils import network\n",
        "network.measure_network_speed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rlj5od9V88o"
      },
      "source": [
        "#### Memory size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ne4-jAiPV88o",
        "outputId": "808a882d-d0fc-4b95-f152-16ffbce0680e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total memory size: 12.68 GB\n",
            "Available memory size: 11.48 GB\n",
            "Used memory size: 0.88 GB\n"
          ]
        }
      ],
      "source": [
        "from src.utils import memory\n",
        "memory.show_memory_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvqEI_geV88o"
      },
      "source": [
        "## 3. Load data from Google Cloud Storage by dask dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qclBRdh7V88p"
      },
      "source": [
        "### Columns of camera_image\n",
        "\n",
        "camera_image: CameraImageComponent\n",
        "- string                                   key.segment_context_name\n",
        "- int64                                    key.frame_timestamp_micros\n",
        "- int8                                     key.camera_name\n",
        "- binary                                   [CameraImageComponent].image (336693 bytes)\n",
        "- fixed_size_list<item: double>[16]        [CameraImageComponent].pose.transform\n",
        "- float                                    [CameraImageComponent].velocity.linear_velocity.x\n",
        "- float                                    [CameraImageComponent].velocity.linear_velocity.y\n",
        "- float                                    [CameraImageComponent].velocity.linear_velocity.z\n",
        "- double                                   [CameraImageComponent].velocity.angular_velocity.x\n",
        "- double                                   [CameraImageComponent].velocity.angular_velocity.y\n",
        "- double                                   [CameraImageComponent].velocity.angular_velocity.z\n",
        "- double                                   [CameraImageComponent].pose_timestamp\n",
        "- double                                   [CameraImageComponent].rolling_shutter_params.shutter\n",
        "- double                                   [CameraImageComponent].rolling_shutter_params.camera_trigger_time\n",
        "- double                                   [CameraImageComponent].rolling_shutter_params.camera_readout_done_time\n",
        "\n",
        "### Columns of comera_box\n",
        "\n",
        "camera_box: CameraBoxComponent\n",
        "- string                                   key.segment_context_name\n",
        "- int64                                    key.frame_timestamp_micros\n",
        "- int8                                     key.camera_name\n",
        "- string                                   key.camera_object_id\n",
        "- double                                   [CameraBoxComponent].box.center.x\n",
        "- double                                   [CameraBoxComponent].box.center.y\n",
        "- double                                   [CameraBoxComponent].box.size.x\n",
        "- double                                   [CameraBoxComponent].box.size.y\n",
        "- int8                                     [CameraBoxComponent].type\n",
        "- int8                                     [CameraBoxComponent].difficulty_level.detection\n",
        "- int8                                     [CameraBoxComponent].difficulty_level.tracking\n",
        "\n",
        "### Columns of camera_calibration\n",
        "\n",
        "camera_calibration: CameraCalibrationComponent\n",
        "- string                                   key.segment_context_name\n",
        "- int8                                     key.camera_name\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.f_u\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.f_v\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.c_u\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.c_v\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.k1\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.k2\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.p1\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.p2\n",
        "- double                                   [CameraCalibrationComponent].intrinsic.k3\n",
        "- fixed_size_list<item: double>[16]        [CameraCalibrationComponent].extrinsic.transform\n",
        "- int32                                    [CameraCalibrationComponent].width\n",
        "- int32                                    [CameraCalibrationComponent].height\n",
        "- int8                                     [CameraCalibrationComponent].rolling_shutter_direction\n",
        "\n",
        "### Columns of stats\n",
        "\n",
        "stats: StatsComponent\n",
        "- string                                   key.segment_context_name\n",
        "- int64                                    key.frame_timestamp_micros\n",
        "- string                                   [StatsComponent].time_of_day\n",
        "- string                                   [StatsComponent].location\n",
        "- string                                   [StatsComponent].weather\n",
        "- list<item: int8>                         [StatsComponent].lidar_object_counts.types\n",
        "- list<item: int32>                        [StatsComponent].lidar_object_counts.counts\n",
        "- list<item: int8>                         [StatsComponent].camera_object_counts.types\n",
        "- list<item: int32>                        [StatsComponent].camera_object_counts.counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_tY7vMgPV88q",
        "outputId": "dc9108d3-3931-4db1-85e7-40bb3c448688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab => Use colab authentication\n"
          ]
        }
      ],
      "source": [
        "from src.data import load_gcs_parquet\n",
        "# 3.1 Get GCSFileSystem\n",
        "GCSFileSystem = load_gcs_parquet.get_GCSFileSystem(use_colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LiCPG27JV88q"
      },
      "outputs": [],
      "source": [
        "# Define columns to load\n",
        "# Set columns to None to load all columns\n",
        "columns_camera_image = ['key.segment_context_name', 'key.frame_timestamp_micros', 'key.camera_name', '[CameraImageComponent].image']\n",
        "columns_camera_box = ['key.segment_context_name', 'key.frame_timestamp_micros', 'key.camera_name', 'key.camera_object_id',\n",
        "                      '[CameraBoxComponent].box.center.x', '[CameraBoxComponent].box.center.y', '[CameraBoxComponent].box.size.x', '[CameraBoxComponent].box.size.y',\n",
        "                      '[CameraBoxComponent].type', '[CameraBoxComponent].difficulty_level.detection', '[CameraBoxComponent].difficulty_level.tracking']\n",
        "#* columns_camera_calibration = None\n",
        "#* columns_stats = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mo2jAAfzV88q",
        "outputId": "ceb6f52e-4223-4015-9379-bb2124f42299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data waymo_open_dataset_v_2_0_0/validation/camera_image/*.parquet from GCS...\n",
            "[########################################] | 100% Completed | 32.88 s\n",
            "CPU times: user 5.1 s, sys: 502 ms, total: 5.6 s\n",
            "Wall time: 34.8 s\n",
            "Loading data waymo_open_dataset_v_2_0_0/validation/camera_box/*.parquet from GCS...\n",
            "CPU times: user 37.2 ms, sys: 3.17 ms, total: 40.4 ms\n",
            "Wall time: 224 ms\n"
          ]
        }
      ],
      "source": [
        "from dask import diagnostics\n",
        "diagnostics.ProgressBar().register()\n",
        "# Define the size of each dask dataframe partition\n",
        "partition_size = None # 128MB is the default size\n",
        "# partition_size = '64MB'\n",
        "\n",
        "# 3.2 Loading data waymo_open_dataset_v_2_0_0/training/camera_image/*.parquet from GCS\n",
        "# Wall time: 2min 24s\n",
        "#* %time df_training_camera_image = load_gcs_parquet.get_df('training', 'camera_image', GCSFileSystem, columns=columns_camera_image, partition_size=partition_size)\n",
        "\n",
        "# 3.3 Loading data waymo_open_dataset_v_2_0_0/training/camera_box/*.parquet from GCS\n",
        "# Wall time: 1.69 s\n",
        "#* %time df_training_camera_box = load_gcs_parquet.get_df('training', 'camera_box', GCSFileSystem, columns=columns_camera_box, partition_size=partition_size)\n",
        "\n",
        "# 3.4 Loading data waymo_open_dataset_v_2_0_0/training/camera_calibration/*.parquet from GCS\n",
        "# Wall time: 1.85 s\n",
        "#* %time df_training_camera_calibration = load_gcs_parquet.get_df('training', 'camera_calibration', GCSFileSystem, columns=columns_camera_calibration, partition_size=partition_size)\n",
        "\n",
        "# 3.5 Loading data waymo_open_dataset_v_2_0_0/training/stats/*.parquet from GCS\n",
        "# Wall time: 1.8 s\n",
        "#* %time df_training_stats = load_gcs_parquet.get_df('training', 'stats', GCSFileSystem, columns=columns_stats, partition_size=partition_size)\n",
        "\n",
        "# 3.6 Loading data waymo_open_dataset_v_2_0_0/validation/camera_image/*.parquet from GCS\n",
        "\n",
        "%time df_validation_camera_image = load_gcs_parquet.get_df('validation', 'camera_image', GCSFileSystem, columns=columns_camera_image, partition_size=partition_size)\n",
        "\n",
        "# 3.7 Loading data waymo_open_dataset_v_2_0_0/validation/camera_box/*.parquet from GCS\n",
        "\n",
        "%time df_validation_camera_box = load_gcs_parquet.get_df('validation', 'camera_box', GCSFileSystem, columns=columns_camera_box, partition_size=partition_size)\n",
        "\n",
        "# 3.8 Loading data waymo_open_dataset_v_2_0_0/validation/camera_calibration/*.parquet from GCS\n",
        "#* %time df_validation_camera_calibration = load_gcs_parquet.get_df('validation', 'camera_calibration', GCSFileSystem, , columns=columns_camera_calibration, partition_size=partition_size)\n",
        "\n",
        "# 3.9 Loading data waymo_open_dataset_v_2_0_0/validation/stats/*.parquet from GCS\n",
        "#* %time df_validation_stats = load_gcs_parquet.get_df('validation', 'stats', GCSFileSystem, partition_size=partition_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpz3a49GV88r"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG-YuMGeV88r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgCFfRI9V88s"
      },
      "source": [
        "## 5. Create Submission\n",
        "reference: [Waymo Open Dataset 3D Camera-Only Detection Tutorial](https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial_camera_only.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEUQVBOhV88s"
      },
      "outputs": [],
      "source": [
        "# from src.submit import format\n",
        "\n",
        "# # Prepare predictions. Please modify accordingly to process your inference results.\n",
        "# context_names = ['1305342127382455702_3720_000_3740_000']\n",
        "\n",
        "# frame_timestamps = {\n",
        "#     # Please make sure that the timestamps match frame.timestamp_micros.\n",
        "#     '1305342127382455702_3720_000_3740_000': [1511019682029265, 1511019682129243]\n",
        "# }\n",
        "\n",
        "# prediction_objects = {}\n",
        "# for context_name in context_names:\n",
        "#   prediction_objects[context_name] = {}\n",
        "#   for timestamp in frame_timestamps[context_name]:\n",
        "#     # Create objects based on inference results\n",
        "#     prediction_objects[context_name][timestamp] = format.make_inference_objects(\n",
        "#         context_name=context_name,\n",
        "#         timestamp=timestamp,\n",
        "#         boxes=np.random.rand(3, 4),\n",
        "#         classes=np.random.randint(low=1, high=4, size=(3,)),\n",
        "#         scores=np.random.rand(3,),\n",
        "#         camera_names=np.random.randint(low=1, high=5, size=(3,))\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooJFXSgJV88s"
      },
      "outputs": [],
      "source": [
        "# context_name = context_names[0]\n",
        "# timestamp = frame_timestamps[context_name][0]\n",
        "# print(prediction_objects[context_name][timestamp][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7TJoSmRV88t"
      },
      "outputs": [],
      "source": [
        "# test_info = format.test_submit_info()\n",
        "# print(test_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBcYus43V88t"
      },
      "outputs": [],
      "source": [
        "# submission_file_base='./MySubmission'\n",
        "\n",
        "# format.pack_to_submission(\n",
        "#     submission_file_base=submission_file_base,\n",
        "#     prediction_objects=prediction_objects,\n",
        "#     **test_info \n",
        "# )\n",
        "# print(f'Then you can upload {submission_file_base}.tar.gz to the challenge website.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8raAgXXV88u"
      },
      "source": [
        "## Make a ground truth validation answer for testing the submition format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wou_l2E1V88u"
      },
      "outputs": [],
      "source": [
        "from src.submit import format as format_\n",
        "import numpy as np\n",
        "\n",
        "def make_ground_truth_answer(partition_dataframe):\n",
        "    \n",
        "    prediction_objects = {}\n",
        "    \n",
        "    for index, row in partition_dataframe.iterrows():\n",
        "      context_name = row['key.segment_context_name']\n",
        "      timestamp = row['key.frame_timestamp_micros']\n",
        "      if context_name not in prediction_objects:\n",
        "        prediction_objects[context_name] = {}\n",
        "      if timestamp not in prediction_objects[context_name]:\n",
        "        prediction_objects[context_name][timestamp] = []\n",
        "      \n",
        "      prediction_objects[context_name][timestamp].append(\n",
        "        format_.make_inference_objects(\n",
        "            context_name=context_name,\n",
        "            timestamp=timestamp,\n",
        "            boxes=np.array(\n",
        "                [\n",
        "                    [\n",
        "                        row['[CameraBoxComponent].box.center.x'],\n",
        "                        row['[CameraBoxComponent].box.center.y'],\n",
        "                        row['[CameraBoxComponent].box.size.x'],\n",
        "                        row['[CameraBoxComponent].box.size.y'],\n",
        "                    ]\n",
        "                ]\n",
        "            ),\n",
        "            classes=np.array([row['[CameraBoxComponent].type']]),\n",
        "            scores=np.array([1.0]),\n",
        "            camera_names=np.array([row['key.camera_name']])\n",
        "        )[0]\n",
        "      )\n",
        "    return prediction_objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYAd2HBTV88u"
      },
      "outputs": [],
      "source": [
        "answers = df_validation_camera_box.map_partitions(make_ground_truth_answer)\n",
        "%time t = answers.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRbqsIqzV88u"
      },
      "outputs": [],
      "source": [
        "test_info = format_.test_submit_info()\n",
        "print(test_info)\n",
        "\n",
        "if use_colab:\n",
        "    submission_file_base='/content/submit_testing_validation_v0'\n",
        "else:\n",
        "    submission_file_base='/data/submit_testing_validation_v0'\n",
        "\n",
        "format_.pack_to_submission(\n",
        "    submission_file_base=submission_file_base,\n",
        "    prediction_objects=t,\n",
        "    num_submission_shards=1,\n",
        "    **test_info \n",
        ")\n",
        "\n",
        "print(f'Then you can upload {submission_file_base}.tar.gz to the challenge website.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ZMGbL-V88v"
      },
      "source": [
        "## 6. Compute Metrics\n",
        "- reference: \n",
        "    - [Waymo Open Dataset 3D Camera-Only Detection Tutorial](https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial_camera_only.ipynb)\n",
        "    - [2D Detection](https://waymo.com/intl/en_us/open/challenges/2020/2d-detection/)\n",
        "        - Metrics:\n",
        "            - Primary metric: Average Precision (AP): \n",
        "                - âˆ«p(r)dr where p(r)is the PR curve\n",
        "            - IoU Overlap Threshold:\n",
        "                - Vehicle 0.7, Pedestrian 0.5, Cyclist 0.5, Sign 0.5\n",
        "            - Sensor Names:\n",
        "                - C: All cameras\n",
        "                - I: Invalid\n",
        "            - Label Difficulty Breakdown:\n",
        "                - Each ground truth label is categorized into different difficulty levels (two levels for now):\n",
        "                    - LEVEL_1, if not marked as LEVEL_2 in the released data.\n",
        "                    - LEVEL_2, if marked as LEVEL_2 in the released data. When evaluating, LEVEL_2 metrics are computed by considering both LEVEL_1 and LEVEL_2 ground truth.\n",
        "            - Metric Breakdown:\n",
        "                - The following metric breakdowns are supported:\n",
        "                    - OBJECT_TYPE: Breakdown by object type (\"ALL_NS\" refers to all objects except signs: Vehicle, Cyclist, and Pedestrian)\n",
        "                    - RANGE: Breakdown by the distance between object center and vehicle frame origin. [0, 35m), [35m, 50m), [50m, +inf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bJELR8LV88v"
      },
      "outputs": [],
      "source": [
        "from waymo_open_dataset.metrics.python import wod_detection_evaluator\n",
        "from src.evaluation import metrics\n",
        "\n",
        "config = metrics.get_config()\n",
        "evaluator = wod_detection_evaluator.WODDetectionEvaluator(config=config)\n",
        "# print(evaluator._config)\n",
        "\n",
        "# for _ in range(num_evals):\n",
        "#     for _ in range(num_batches_per_eval):\n",
        "#         predictions, groundtruth = predictor.predict(...)  # pop a batch.\n",
        "#         evaluator.update_state(groundtruths, predictions)\n",
        "# evaluator.result()  # finish one full eval and reset states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InfqxUvYV88v"
      },
      "source": [
        "## Test evalueation by fake files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzSgCGAlV88w"
      },
      "outputs": [],
      "source": [
        "from waymo_open_dataset import label_pb2\n",
        "from waymo_open_dataset.protos import metrics_pb2\n",
        "from waymo_open_dataset.metrics.ops import py_metrics_ops\n",
        "from waymo_open_dataset.metrics.python import config_util_py as config_util\n",
        "import tensorflow as tf\n",
        "\n",
        "def compute_let_detection_metrics(prediction_frame_id,\n",
        "                                  prediction_bbox,\n",
        "                                  prediction_type,\n",
        "                                  prediction_score,\n",
        "                                  ground_truth_frame_id,\n",
        "                                  ground_truth_bbox,\n",
        "                                  ground_truth_type,\n",
        "                                  ground_truth_difficulty,\n",
        "                                  recall_at_precision=None,\n",
        "                                  name_filter=None,\n",
        "                                  config=metrics.get_config()):\n",
        "  \"\"\"Returns dict of metric name to metric values`.\n",
        "\n",
        "  Notation:\n",
        "    * M: number of predicted boxes.\n",
        "    * D: number of box dimensions. The number of box dimensions can be one of\n",
        "         the following:\n",
        "           4: Used for boxes with type TYPE_AA_2D (center_x, center_y, length,\n",
        "              width)\n",
        "           5: Used for boxes with type TYPE_2D (center_x, center_y, length,\n",
        "              width, heading).\n",
        "           7: Used for boxes with type TYPE_3D (center_x, center_y, center_z,\n",
        "              length, width, height, heading).\n",
        "    * N: number of ground truth boxes.\n",
        "\n",
        "  Args:\n",
        "    prediction_frame_id: [M] int64 tensor that identifies frame for each\n",
        "      prediction.\n",
        "    prediction_bbox: [M, D] tensor encoding the predicted bounding boxes.\n",
        "    prediction_type: [M] tensor encoding the object type of each prediction.\n",
        "    prediction_score: [M] tensor encoding the score of each prediciton.\n",
        "    ground_truth_frame_id: [N] int64 tensor that identifies frame for each\n",
        "      ground truth.\n",
        "    ground_truth_bbox: [N, D] tensor encoding the ground truth bounding boxes.\n",
        "    ground_truth_type: [N] tensor encoding the object type of each ground truth.\n",
        "    ground_truth_difficulty: [N] tensor encoding the difficulty level of each\n",
        "      ground truth.\n",
        "    config: The metrics config defined in protos/metrics.proto.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of metric names to metrics values.\n",
        "  \"\"\"\n",
        "  num_ground_truths = tf.shape(ground_truth_bbox)[0]\n",
        "  num_predictions = tf.shape(prediction_bbox)[0]\n",
        "  ground_truth_speed = tf.zeros((num_ground_truths, 2), tf.float32)\n",
        "  prediction_overlap_nlz = tf.zeros((num_predictions), tf.bool)\n",
        "\n",
        "  config_str = config.SerializeToString()\n",
        "  ap, aph, apl, pr, _, _, _ = py_metrics_ops.detection_metrics(\n",
        "      prediction_frame_id=tf.cast(prediction_frame_id, tf.int64),\n",
        "      prediction_bbox=tf.cast(prediction_bbox, tf.float32),\n",
        "      prediction_type=tf.cast(prediction_type, tf.uint8),\n",
        "      prediction_score=tf.cast(prediction_score, tf.float32),\n",
        "      prediction_overlap_nlz=prediction_overlap_nlz,\n",
        "      ground_truth_frame_id=tf.cast(ground_truth_frame_id, tf.int64),\n",
        "      ground_truth_bbox=tf.cast(ground_truth_bbox, tf.float32),\n",
        "      ground_truth_type=tf.cast(ground_truth_type, tf.uint8),\n",
        "      ground_truth_difficulty=tf.cast(ground_truth_difficulty, tf.uint8),\n",
        "      ground_truth_speed=ground_truth_speed,\n",
        "      config=config_str)\n",
        "  breakdown_names = config_util.get_breakdown_names_from_config(config)\n",
        "  metric_values = {}\n",
        "  for i, name in enumerate(breakdown_names):\n",
        "    if name_filter is not None and name_filter not in name:\n",
        "      continue\n",
        "    # Average Precision\n",
        "    metric_values['{}/LET-mAP'.format(name)] = ap[i]\n",
        "    # Average Precision Weighted by Heading\n",
        "    # metric_values['{}/LET-mAPH'.format(name)] = aph[i]\n",
        "    # metric_values['{}/LET-mAPL'.format(name)] = apl[i]\n",
        "    \n",
        "  return metric_values\n",
        "\n",
        "\n",
        "def parse_metrics_objects_binary_files(ground_truths_path, predictions_path):\n",
        "  with tf.io.gfile.GFile(ground_truths_path, 'rb') as f:\n",
        "    ground_truth_objects = metrics_pb2.Objects.FromString(f.read())\n",
        "  with tf.io.gfile.GFile(predictions_path, 'rb') as f:\n",
        "    predictions_objects = metrics_pb2.Objects.FromString(f.read())\n",
        "  eval_dict = {\n",
        "      'prediction_frame_id': [],\n",
        "      'prediction_bbox': [],\n",
        "      'prediction_type': [],\n",
        "      'prediction_score': [],\n",
        "      'ground_truth_frame_id': [],\n",
        "      'ground_truth_bbox': [],\n",
        "      'ground_truth_type': [],\n",
        "      'ground_truth_difficulty': [],\n",
        "  }\n",
        "\n",
        "  # Parse and filter ground truths.\n",
        "  for obj in ground_truth_objects.objects:\n",
        "    # Ignore objects that are not in Cameras' FOV.\n",
        "    if not obj.object.most_visible_camera_name:\n",
        "      continue\n",
        "    # Ignore objects that are fully-occluded to cameras.\n",
        "    if obj.object.num_lidar_points_in_box == 0:\n",
        "      continue\n",
        "    # Fill in unknown difficulties.\n",
        "    if obj.object.detection_difficulty_level == label_pb2.Label.UNKNOWN:\n",
        "      obj.object.detection_difficulty_level = label_pb2.Label.LEVEL_2\n",
        "    eval_dict['ground_truth_frame_id'].append(obj.frame_timestamp_micros)\n",
        "    # Note that we use `camera_synced_box` for evaluation.\n",
        "    ground_truth_box = obj.object.camera_synced_box\n",
        "    eval_dict['ground_truth_bbox'].append(\n",
        "        np.asarray([\n",
        "            ground_truth_box.center_x,\n",
        "            ground_truth_box.center_y,\n",
        "            # ground_truth_box.center_z,\n",
        "            ground_truth_box.length,\n",
        "            ground_truth_box.width,\n",
        "            # ground_truth_box.height,\n",
        "            # ground_truth_box.heading,\n",
        "        ], np.float32))\n",
        "    eval_dict['ground_truth_type'].append(obj.object.type)\n",
        "    eval_dict['ground_truth_difficulty'].append(\n",
        "        np.uint8(obj.object.detection_difficulty_level))\n",
        "\n",
        "  # Parse predictions.\n",
        "  for obj in predictions_objects.objects:\n",
        "    eval_dict['prediction_frame_id'].append(obj.frame_timestamp_micros)\n",
        "    prediction_box = obj.object.box\n",
        "    eval_dict['prediction_bbox'].append(\n",
        "        np.asarray([\n",
        "            prediction_box.center_x,\n",
        "            prediction_box.center_y,\n",
        "            # prediction_box.center_z,\n",
        "            prediction_box.length,\n",
        "            prediction_box.width,\n",
        "            # prediction_box.height,\n",
        "            # prediction_box.heading,\n",
        "        ], np.float32))\n",
        "    eval_dict['prediction_type'].append(obj.object.type)\n",
        "    eval_dict['prediction_score'].append(obj.score)\n",
        "\n",
        "  for key, value in eval_dict.items():\n",
        "    eval_dict[key] = tf.stack(value)\n",
        "  return eval_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clsCLH6LV88w"
      },
      "outputs": [],
      "source": [
        "WAYMO_OPEN_DATASET_DIR = '/data/fake/'\n",
        "FAKE_GROUND_TRUTHS_BIN = (\n",
        "    WAYMO_OPEN_DATASET_DIR + 'fake_ground_truths.bin')\n",
        "FAKE_PREDICTIONS_BIN = (\n",
        "    WAYMO_OPEN_DATASET_DIR + 'fake_predictions.bin')\n",
        "\n",
        "eval_dict = parse_metrics_objects_binary_files(FAKE_GROUND_TRUTHS_BIN,\n",
        "                                               FAKE_PREDICTIONS_BIN)\n",
        "\n",
        "print(f\"eval_dict\")\n",
        "for key in eval_dict.keys():\n",
        "  print(f\" {key}\")\n",
        "  print(f\"    type: {type(eval_dict[key])}\")\n",
        "  print(f\"    shape: {eval_dict[key].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpnmlG7VV88w"
      },
      "outputs": [],
      "source": [
        "metrics_dict = compute_let_detection_metrics(**eval_dict)\n",
        "for key, value in metrics_dict.items():\n",
        "  print(f'{key:<55}: {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7w5yv0OV88x"
      },
      "source": [
        "## Cancel resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qrbfQFvV88x"
      },
      "outputs": [],
      "source": [
        "client.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}